import os
from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.embeddings import CacheBackedEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
from dotenv import load_dotenv

load_dotenv()


if "logged_in" not in st.session_state or not st.session_state.logged_in:
    st.error("ğŸš« ë¡œê·¸ì¸í•´ì•¼ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. Home ìœ¼ë¡œ ê°€ì„¸ìš”.")
    # st.markdown("[Back to Login](./)")
    st.markdown("""
    <a href="./" target="_self">ğŸ”™ Homeìœ¼ë¡œ ê°€ì„œ ë¡œê·¸ì¸í•˜ê¸°</a>
""", unsafe_allow_html=True)

    st.stop()

# âœ… í´ë” ì•ˆì˜ ëª¨ë“  PDF ë¬¸ì„œë¥¼ ëŒ€ìƒ
STATIC_DIR = Path("./static/guidance")

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

# âœ… ì—¬ëŸ¬ ë¬¸ì„œ ë¡œë”© & ë²¡í„° ìƒì„±
@st.cache_resource(show_spinner="ë¬¸ì„œ ë¡œë”©ì¤‘. ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì„¸ìš”...")
def load_all_documents():
    all_docs = []
    for file_path in STATIC_DIR.glob("*.pdf"):
        loader = PyMuPDFLoader(str(file_path))
        raw_docs = loader.load()

        # ê° ë¬¸ì„œì— file_name ì¶”ê°€
        for doc in raw_docs:
            doc.metadata["source"] = file_path.name

        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n", chunk_size=600, chunk_overlap=100,
        )
        chunks = splitter.split_documents(raw_docs)
        all_docs.extend(chunks)

    # ë²¡í„°í™”
    store = LocalFileStore(f"./.cache/embeddings/all_static/guidance")
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store)
    vectorstore = FAISS.from_documents(all_docs, cached_embeddings)
    print("ğŸ“„ ë¬¸ì„œ ì„ë² ë”© ì‹¤í–‰ë¨")
    return vectorstore.as_retriever()

def save_message(message, role):
    st.session_state["messages2"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages2"]:
        send_message(message["message"], message["role"], save=False)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

prompt = ChatPromptTemplate.from_messages(
    [
           ("system", "Answer the question using ONLY the following context. If you don't know the answer just say you don't know in Korean. DON'T make anything up.\n\nContext: {context}"),
        ("human", "{question}"),
    ]
)

# âœ… Streamlit ì‹œì‘
st.set_page_config(page_title="í•™êµìƒí™œê·œì • ì±—ë´‡", page_icon="ğŸ“‚")
st.title("ğŸ“‚ í•™êµìƒí™œê·œì • ì±—ë´‡")
st.markdown("##### ê°ì¢… í•™êµí­ë ¥ì‚¬ì•ˆì— ëŒ€í•´ ë¬¸ì„œ(**í•™êµìƒí™œê·œì • ê¸¸ë¼ì¡ì´, í•™ìƒìƒí™œì§€ë„ì— ê´€í•œ ê³ ì‹œ**)ë¥¼ ê·¼ê±°ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤.")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages2" not in st.session_state:
    st.session_state["messages2"] = []

# ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
retriever = load_all_documents()
send_message("ëª¨ë“  ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”.", "ai", save=False)
paint_history()

message = st.chat_input("Ask about the documents...")
if message:
    send_message(message, "human")
    # relevant_docs = retriever.get_relevant_documents(message)
    relevant_docs = retriever.invoke(message)


    chain = (
        {
            "context": lambda _: format_docs(relevant_docs),
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )

    with st.chat_message("ai"):
        chain.invoke(message)

    # âœ… ê° ë¬¸ì„œ + í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ
    with st.expander("ğŸ” ì°¸ê³ ë¬¸í—Œ", expanded=False):
        for i, doc in enumerate(relevant_docs, start=1):
            source = doc.metadata.get("source", "Unknown")
            page = doc.metadata.get("page", "â“")
            st.markdown(f"**ë¬¸ì„œ {i} â€” ğŸ“„ `{source}` | Page {page+1}**")
            st.code(doc.page_content.strip(), language="markdown")
