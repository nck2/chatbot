import os
from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader
from langchain.embeddings import CacheBackedEmbeddings
# from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI


from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
# from langchain_community.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st



# def get_api_key():
#     # st.secrets.getì€ None ë°˜í™˜í•˜ë¯€ë¡œ ì•ˆì „
#     cloud_key = st.secrets["OPENAI_API_KEY"]
#     if cloud_key:
#         return cloud_key

#     # ë¡œì»¬ìš© .env í™˜ê²½ ë³€ìˆ˜
#     from dotenv import load_dotenv
#     load_dotenv()
#     return os.getenv("OPENAI_API_KEY")


# api_key = get_api_key()



if "logged_in" not in st.session_state or not st.session_state.logged_in:
    st.error("ğŸš« ë¡œê·¸ì¸í•´ì•¼ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. Home ìœ¼ë¡œ ê°€ì„¸ìš”.")
    # st.markdown("[Back to Login](./)")
    st.markdown("""
    <a href="./" target="_self">ğŸ”™ Homeìœ¼ë¡œ ê°€ì„œ ë¡œê·¸ì¸í•˜ê¸°</a>
""", unsafe_allow_html=True)

    st.stop()

# âœ… í´ë” ì•ˆì˜ ëª¨ë“  PDF ë¬¸ì„œë¥¼ ëŒ€ìƒ
STATIC_DIR = Path("./static/useful")

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],

)

# âœ… ì—¬ëŸ¬ ë¬¸ì„œ ë¡œë”© & ë²¡í„° ìƒì„±
@st.cache_resource(show_spinner="ë¬¸ì„œ ë¡œë”©ì¤‘. ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì„¸ìš”...")
def load_all_documents():
    all_docs = []
    for file_path in STATIC_DIR.glob("*.pdf"):
        # loader = PyMuPDFLoader(str(file_path))
        loader = PDFPlumberLoader(str(file_path))

        raw_docs = loader.load()


        # ê° ë¬¸ì„œì— file_name ì¶”ê°€
        for doc in raw_docs:
            doc.metadata["source"] = file_path.name

        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n", chunk_size=800, chunk_overlap=100,
        )
        chunks = splitter.split_documents(raw_docs)
        all_docs.extend(chunks)

    # ë²¡í„°í™”
    store = LocalFileStore(f"./.cache/embeddings/all_static/useful")
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store)
    vectorstore = FAISS.from_documents(all_docs, cached_embeddings)
    print("ğŸ“„ ë¬¸ì„œ ì„ë² ë”© ì‹¤í–‰ë¨")
    # return vectorstore.as_retriever()
    return vectorstore.as_retriever(search_kwargs={"k": 8})


def save_message(message, role):
    st.session_state["messages4"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages4"]:
        send_message(message["message"], message["role"], save=False)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

prompt = ChatPromptTemplate.from_messages(
    [
           ("system", "Answer the question using ONLY the following context. If you don't know the answer just say you don't know in Korean. DON'T make anything up.\n\nContext: {context}"),
        ("human", "{question}"),
    ]
)

# âœ… Streamlit ì‹œì‘
st.set_page_config(page_title="ì•Œì“¸ì†Œì¤‘ ì±—ë´‡", page_icon="ğŸ“‚")
st.title("ğŸ“‚ ì•Œì“¸ì†Œì¤‘ ì±—ë´‡")
st.markdown("##### **ì•Œì“¸ì†Œì¤‘** ì„ ê·¼ê±°ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤.")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages4" not in st.session_state:
    st.session_state["messages4"] = []



# ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
retriever = load_all_documents()
send_message("ëª¨ë“  ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”.", "ai", save=False)
paint_history()

message = st.chat_input("Ask about the documents...")
if message:
    send_message(message, "human")
    # relevant_docs = retriever.get_relevant_documents(message)

    docs_with_scores = retriever.vectorstore.similarity_search_with_score(message, k=10)
    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 4ê°œë§Œ ì„ íƒ (ì ìˆ˜ ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ)
    top_docs = sorted(docs_with_scores, key=lambda x: x[1])[:4]
    # ë¬¸ì„œë§Œ ì¶”ì¶œ
    relevant_docs = [doc for doc, score in top_docs]



    chain = (
        {
            "context": lambda _: format_docs(relevant_docs),
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )

    with st.chat_message("ai"):
        chain.invoke(message)

      # âœ… ê° ë¬¸ì„œ + í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ
    with st.expander("ğŸ” ì°¸ê³ ë¬¸í—Œ", expanded=False):
        for i, (doc, score) in enumerate(top_docs, start=1):
            source = doc.metadata.get("source", "Unknown")
            page = doc.metadata.get("page", "â“")
            st.markdown(f"**ë¬¸ì„œ {i} â€” ğŸ“„ `{source}` | Page {page+1} | ìœ ì‚¬ë„: {score:.4f}**")
            st.code(doc.page_content.strip(), language="markdown")
