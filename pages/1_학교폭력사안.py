import os
from pathlib import Path
from dotenv import load_dotenv

import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.embeddings import CacheBackedEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler

# âœ… ì„¤ì •
STATIC_DIR = Path("./static/schoolviolence")

# âœ… ë¡œê·¸ì¸ í™•ì¸
if "logged_in" not in st.session_state or not st.session_state.logged_in:
    st.error("ğŸš« ë¡œê·¸ì¸í•´ì•¼ ì‚¬ìš©ê°€ëŠ¥í•©ë‹ˆë‹¤. Home ìœ¼ë¡œ ê°€ì„¸ìš”.")
    st.markdown("""
        <a href="./" target="_self">ğŸ”™ Homeìœ¼ë¡œ ê°€ì„œ ë¡œê·¸ì¸í•˜ê¸°</a>
    """, unsafe_allow_html=True)
    st.stop()

# âœ… ì½œë°± í•¸ë“¤ëŸ¬ (ìŠ¤íŠ¸ë¦¬ë°)
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

# âœ… LLM ì„¤ì •
llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

# âœ… ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°í™”
@st.cache_resource(show_spinner="ë¬¸ì„œ ë¡œë”©ì¤‘. ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì„¸ìš”...")
def load_all_documents():
    all_docs = []
    for file_path in STATIC_DIR.glob("*.pdf"):
        loader = PyMuPDFLoader(str(file_path))
        raw_docs = loader.load()

        for doc in raw_docs:
            doc.metadata["source"] = file_path.name

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", ".", " ", ""],
        )
        chunks = splitter.split_documents(raw_docs)
        all_docs.extend(chunks)

    store = LocalFileStore(f"./.cache/embeddings/all_static/schoolviolence")
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, store)
    vectorstore = FAISS.from_documents(all_docs, cached_embeddings)

    return vectorstore  # as_retriever ëŒ€ì‹  ì§ì ‘ vectorstore ë°˜í™˜

# âœ… ë©”ì‹œì§€ ì €ì¥/ì „ì†¡
def save_message(message, role):
    st.session_state["messages1"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages1"]:
        send_message(message["message"], message["role"], save=False)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a supervisor at the Seoul Metropolitan Office of Education, responsible for school violence prevention and student life guidance.
Answer questions based strictly on the provided context.
Your answers must follow these guidelines:
- Provide accurate, detailed, and rich information
- Use clear and professional language
- If the information is not available in the context, respond with: "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
DON'T make anything up.\n\nContext: {context}"""),
    ("human", "{question}"),
])

# âœ… í˜ì´ì§€ UI ì„¤ì •
st.set_page_config(page_title="í•™êµí­ë ¥ì‚¬ì•ˆ ì±—ë´‡", page_icon="ğŸ“‚")
st.title("ğŸ“‚ í•™êµí­ë ¥ì‚¬ì•ˆ ì±—ë´‡")
st.markdown("##### ê°ì¢… í•™êµí­ë ¥ì‚¬ì•ˆì— ëŒ€í•´ ë¬¸ì„œ(**ì‚¬ì•ˆì²˜ë¦¬ê°€ì´ë“œë¶**)ë¥¼ ê·¼ê±°ë¡œ ëŒ€ë‹µí•©ë‹ˆë‹¤.")

if "messages1" not in st.session_state:
    st.session_state["messages1"] = []

# âœ… ë¬¸ì„œ ë¡œë”©
vectorstore = load_all_documents()
send_message("ëª¨ë“  ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•˜ì„¸ìš”.", "ai", save=False)
paint_history()

# âœ… ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
message = st.chat_input("Ask about the documents...")
if message:
    send_message(message, "human")

    docs_with_scores = vectorstore.similarity_search_with_score(message, k=10)
    filtered_docs = [doc for doc, score in sorted(docs_with_scores, key=lambda x: x[1]) if score <= 0.6]
    top_docs = filtered_docs[:4]

    chain = (
        {
            "context": lambda _: format_docs(top_docs),
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )

    with st.chat_message("ai"):
        chain.invoke(message)

    with st.expander("ğŸ” ì°¸ê³ ë¬¸í—Œ", expanded=False):
        for i, doc in enumerate(top_docs, start=1):
            source = doc.metadata.get("source", "Unknown")
            page = doc.metadata.get("page", 0)
            st.markdown(f"**ë¬¸ì„œ {i} â€” ğŸ“„ `{source}` | Page {page + 1} | ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ ìƒìœ„**")
            st.code(doc.page_content.strip(), language="markdown")
